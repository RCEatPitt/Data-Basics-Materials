{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "* Why do we care about Python?\n",
    "* Variables and data types\n",
    "* Control flow - if, else, elif\n",
    "* Data structures - lists, dictionaries\n",
    "* Loops\n",
    "* Functions\n",
    "* Working with text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we care about Python?\n",
    "\n",
    "**Federalist papers**\n",
    "\n",
    "Alexander Hamilton, James Madison, or John Jay?  For more than 150 years, historians argued over the authorship of the 12 essays in _The Federalist Papers_. It wasn't until 1963 that the mystery was solved by Frederick Mosteller of Harvard University and David Wallace of the University of Chicago. [Nabokov's Favorite Word Is _Mauve_ by Ben Blatt]\n",
    "\n",
    "Full text of _The Federalist Papers_ is available at http://www.gutenberg.org/ebooks/1404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to our data file (source file)\n",
    "source_file_name = 'federalist_papers.txt'\n",
    "\n",
    "fed_papers_file = open(source_file_name, 'r')\n",
    "\n",
    "\n",
    "# We can read all text at once\n",
    "all_text = fed_papers_file.read()\n",
    "#print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are a couple of ways we could find frequencies of the words \"while\" and \"whilst\".  \n",
    "# For now, let's convert our chunk of text into a list of words\n",
    "\n",
    "word_list = all_text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will this work?  Are words always separated by spaces?\n",
    "# While there are better methods for dealing with text parsing (for example, nltk toolkit)\n",
    "# for now we'll take care of things in a quick and dirty way\n",
    "\n",
    "punctuation_marks = ['!','.', ',', ':', ';', '?', '-', '\\n']\n",
    "for pm in punctuation_marks:\n",
    "    all_text = all_text.replace(pm, ' ')\n",
    "                     \n",
    "# print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It would be a good idea to convert everything to lower case before we do anything else\n",
    "all_text = all_text.lower()\n",
    "\n",
    "# Now let's build a list of words\n",
    "word_list = all_text.split(\" \")\n",
    "# print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's find the frequency for \"while\"\n",
    "\n",
    "freq_while = 0\n",
    "freq_whilst = 0\n",
    "for word in word_list:\n",
    "    if word == \"while\":\n",
    "        freq_while = freq_while + 1\n",
    "    if word == \"whilst\":\n",
    "        freq_whilst = freq_whilst + 1\n",
    "        \n",
    "print(\"The frequency of 'while' is: \" + str(freq_while))\n",
    "print(\"The frequency of 'whilst' is: \" + str(freq_whilst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
